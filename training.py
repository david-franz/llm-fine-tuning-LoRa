# -*- coding: utf-8 -*-
"""retraining_a2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aRC0D8fMlEDExGnnAeYxEUzGB34we2BB
"""

!pip install transformers peft datasets accelerate

from transformers import AutoTokenizer, AutoModelForCausalLM

BASE_MODEL_NAME = "Qwen/Qwen3-0.6B"

base_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_NAME,
    torch_dtype="auto",
    device_map="auto"
)

from peft import LoraConfig, get_peft_model, TaskType

lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    r=16,
    lora_alpha=32,
    target_modules="all-linear",
    lora_dropout=0,
    bias="none"
)

lora_model = get_peft_model(base_model, lora_config)

from datasets import load_dataset

ds = load_dataset("mhhmm/typescript-instruct-20k", split="train")

def preprocess(examples):
    inputs = examples["instruction"]
    targets = [out + base_tokenizer.eos_token for out in examples["output"]]
    model_inputs = base_tokenizer(inputs, truncation=True, padding="max_length", max_length=2048)
    labels = base_tokenizer(targets, truncation=True, padding="max_length", max_length=2048).input_ids
    model_inputs["labels"] = labels
    return model_inputs

tokenized_ds = ds.map(preprocess, batched=True, remove_columns=ds.column_names)

import os
from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl

class SaveEveryNEpochCallback(TrainerCallback):
    def __init__(self, save_every: int = 1):
        self.save_every = save_every

    def on_epoch_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
        epoch = int(state.epoch)
        if epoch % self.save_every == 0:
            out_dir = os.path.join(args.output_dir, f"checkpoint-epoch{epoch}")
            os.makedirs(out_dir, exist_ok=True)
            model = kwargs["model"]
            model.save_pretrained(out_dir)
            print(f">>> Saved LoRA adapter model to {out_dir}")
        return control

from transformers import Trainer, TrainingArguments
from peft import PeftModel, PeftConfig
import torch

base_tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-0.6B", force_download=True, cache_dir="/tmp/huggingface")
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen3-0.6B",
    torch_dtype="auto",
    device_map="auto",
    force_download=True,
    cache_dir="/tmp/huggingface"
)

lora_adapter_path = "/content/model"

peft_config = PeftConfig.from_pretrained(lora_adapter_path)
lora_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen3-0.6B",
    torch_dtype="auto",
    device_map="auto",
    force_download=True,  # Ensure fresh download
    cache_dir="/tmp/huggingface" # Use temporary cache directory
)
lora_model = PeftModel.from_pretrained(lora_model, lora_adapter_path, config=peft_config)

# **Crucial Change:** Make sure LoRA parameters require gradients
for name, param in lora_model.named_parameters():
    if "lora" in name:  # Target only LoRA parameters
        param.requires_grad = True

# Now proceed with the training loop
training_args = TrainingArguments(
    output_dir="qwen3-ts",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    learning_rate=3e-5,
    fp16=True,
    logging_steps=5,
    save_total_limit=2,
    save_steps=200,
    report_to="none"
)

trainer = Trainer(
    model=lora_model,
    args=training_args,
    train_dataset=tokenized_ds,
    tokenizer=base_tokenizer,
    callbacks = [SaveEveryNEpochCallback(save_every=1)],
)

trainer.train()
# Save only the LoRA adapters
# lora_model.save_pretrained("qwen3-ts")
